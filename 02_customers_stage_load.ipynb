{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1a679ed1-d3d9-4872-af4b-3459291829eb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing customers data from: /Volumes/external-catlog-project/default/incremental_load/customers_data/source/\nStaging table: `external-catlog-project`.default.customers_stage\n"
     ]
    }
   ],
   "source": [
    "# Configuration\n",
    "source_dir = \"/Volumes/external-catlog-project/default/incremental_load/customers_data/source/\"\n",
    "archive_dir = \"/Volumes/external-catlog-project/default/incremental_load/customers_data/archive/\"\n",
    "stage_table = \"`external-catlog-project`.default.customers_stage\"\n",
    "error_table = \"`external-catlog-project`.default.customers_errors\"\n",
    "\n",
    "print(f\"Processing customers data from: {source_dir}\")\n",
    "print(f\"Staging table: {stage_table}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8f48ff26-5bbe-44cf-a16c-be54425fb06a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Schema defined for customers data\n"
     ]
    }
   ],
   "source": [
    "# Import required libraries\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import *\n",
    "from datetime import datetime\n",
    "import json\n",
    "import re\n",
    "\n",
    "# Define schema for customers data\n",
    "customers_schema = StructType([\n",
    "    StructField(\"customer_id\", StringType(), False),\n",
    "    StructField(\"first_name\", StringType(), False),\n",
    "    StructField(\"last_name\", StringType(), False),\n",
    "    StructField(\"email\", StringType(), False),\n",
    "    StructField(\"phone\", StringType(), False),\n",
    "    StructField(\"date_of_birth\", DateType(), False),\n",
    "    StructField(\"registration_date\", DateType(), False),\n",
    "    StructField(\"address\", StringType(), False),\n",
    "    StructField(\"city\", StringType(), False),\n",
    "    StructField(\"state\", StringType(), False),\n",
    "    StructField(\"zip_code\", StringType(), False),\n",
    "    StructField(\"country\", StringType(), False),\n",
    "    StructField(\"customer_tier\", StringType(), False),\n",
    "    StructField(\"last_login\", TimestampType(), False),\n",
    "    StructField(\"created_timestamp\", TimestampType(), False)\n",
    "])\n",
    "\n",
    "print(\"Schema defined for customers data\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bb4cf62d-6000-4560-873f-c0140ef5765f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total records processed: 0\nRecords with null customer_id: 0\nRecords with null email: 0\nRecords with null phone: 0\nRecords with invalid email format: 0\nRecords with invalid phone format: 0\nRecords with future birth dates: 0\nValid records: 0\nInvalid records: 0\n"
     ]
    }
   ],
   "source": [
    "# Read and validate customers data\n",
    "try:\n",
    "    # Read CSV files with schema validation\n",
    "    df_customers = spark.read.schema(customers_schema).csv(source_dir, header=True, dateFormat=\"yyyy-MM-dd\", timestampFormat=\"yyyy-MM-dd HH:mm:ss\")\n",
    "    \n",
    "    # Add processing metadata\n",
    "    df_customers = df_customers.withColumn(\"processed_timestamp\", F.current_timestamp()) \\\n",
    "                              .withColumn(\"batch_id\", F.lit(datetime.now().strftime(\"%Y%m%d_%H%M%S\"))) \\\n",
    "                              .withColumn(\"source_system\", F.lit(\"ecommerce_customers\"))\n",
    "    \n",
    "    # Data quality checks - Simplified validation\n",
    "    total_records = df_customers.count()\n",
    "    null_customer_ids = df_customers.filter(F.col(\"customer_id\").isNull()).count()\n",
    "    null_emails = df_customers.filter(F.col(\"email\").isNull()).count()\n",
    "    null_phones = df_customers.filter(F.col(\"phone\").isNull()).count()\n",
    "    future_birth_dates = df_customers.filter(F.col(\"date_of_birth\") > F.current_date()).count()\n",
    "    \n",
    "    # Simple email validation (contains @ and .)\n",
    "    invalid_emails = df_customers.filter(\n",
    "        (F.col(\"email\").isNotNull()) & \n",
    "        (~F.col(\"email\").contains(\"@\")) | \n",
    "        (~F.col(\"email\").contains(\".\"))\n",
    "    ).count()\n",
    "    \n",
    "    # Simple phone validation (contains - and has 12 characters)\n",
    "    invalid_phones = df_customers.filter(\n",
    "        (F.col(\"phone\").isNotNull()) & \n",
    "        (~F.col(\"phone\").contains(\"-\")) | \n",
    "        (F.length(F.col(\"phone\")) != 12)\n",
    "    ).count()\n",
    "    \n",
    "    print(f\"Total records processed: {total_records}\")\n",
    "    print(f\"Records with null customer_id: {null_customer_ids}\")\n",
    "    print(f\"Records with null email: {null_emails}\")\n",
    "    print(f\"Records with null phone: {null_phones}\")\n",
    "    print(f\"Records with invalid email format: {invalid_emails}\")\n",
    "    print(f\"Records with invalid phone format: {invalid_phones}\")\n",
    "    print(f\"Records with future birth dates: {future_birth_dates}\")\n",
    "    \n",
    "    # Filter out invalid records - Simplified validation\n",
    "    df_valid_customers = df_customers.filter(\n",
    "        (F.col(\"customer_id\").isNotNull()) & \n",
    "        (F.col(\"email\").isNotNull()) & \n",
    "        (F.col(\"phone\").isNotNull()) & \n",
    "        (F.col(\"email\").contains(\"@\")) & \n",
    "        (F.col(\"email\").contains(\".\")) & \n",
    "        (F.col(\"date_of_birth\") <= F.current_date())\n",
    "    )\n",
    "    \n",
    "    # Capture invalid records for error handling - Simplified validation\n",
    "    df_invalid_customers = df_customers.filter(\n",
    "        (F.col(\"customer_id\").isNull()) | \n",
    "        (F.col(\"email\").isNull()) | \n",
    "        (F.col(\"phone\").isNull()) | \n",
    "        (~F.col(\"email\").contains(\"@\")) | \n",
    "        (~F.col(\"email\").contains(\".\")) | \n",
    "        (F.col(\"date_of_birth\") > F.current_date())\n",
    "    )\n",
    "    \n",
    "    valid_records = df_valid_customers.count()\n",
    "    invalid_records = df_invalid_customers.count()\n",
    "    \n",
    "    print(f\"Valid records: {valid_records}\")\n",
    "    print(f\"Invalid records: {invalid_records}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error reading customers data: {str(e)}\")\n",
    "    raise\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c9635b59-2416-4caa-aba5-66070b72daa4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data enrichment completed\n"
     ]
    }
   ],
   "source": [
    "# Data enrichment - Calculate customer age and segment\n",
    "try:\n",
    "    # Calculate age from date of birth\n",
    "    df_valid_customers = df_valid_customers.withColumn(\n",
    "        \"age\", \n",
    "        F.floor(F.months_between(F.current_date(), F.col(\"date_of_birth\")) / 12)\n",
    "    )\n",
    "    \n",
    "    # Create age segments\n",
    "    df_valid_customers = df_valid_customers.withColumn(\n",
    "        \"age_segment\",\n",
    "        F.when(F.col(\"age\") < 25, \"Gen Z\")\n",
    "         .when(F.col(\"age\") < 40, \"Millennial\")\n",
    "         .when(F.col(\"age\") < 55, \"Gen X\")\n",
    "         .otherwise(\"Boomer+\")\n",
    "    )\n",
    "    \n",
    "    # Calculate days since registration\n",
    "    df_valid_customers = df_valid_customers.withColumn(\n",
    "        \"days_since_registration\",\n",
    "        F.datediff(F.current_date(), F.col(\"registration_date\"))\n",
    "    )\n",
    "    \n",
    "    # Create customer lifecycle stage\n",
    "    df_valid_customers = df_valid_customers.withColumn(\n",
    "        \"lifecycle_stage\",\n",
    "        F.when(F.col(\"days_since_registration\") < 30, \"New\")\n",
    "         .when(F.col(\"days_since_registration\") < 365, \"Active\")\n",
    "         .otherwise(\"Established\")\n",
    "    )\n",
    "    \n",
    "    print(\"Data enrichment completed\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error in data enrichment: {str(e)}\")\n",
    "    raise\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "eb008409-9b2d-4987-a5aa-688db75af239",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded 0 valid customers to staging table\n"
     ]
    }
   ],
   "source": [
    "# Write valid data to staging table\n",
    "try:\n",
    "    # Create or overwrite staging table\n",
    "    df_valid_customers.write.format(\"delta\").mode(\"overwrite\").saveAsTable(stage_table)\n",
    "    print(f\"Successfully loaded {valid_records} valid customers to staging table\")\n",
    "    \n",
    "    # Write invalid records to error table for investigation\n",
    "    if invalid_records > 0:\n",
    "        df_invalid_customers.withColumn(\"error_reason\", F.lit(\"Data quality validation failed\")) \\\n",
    "                           .withColumn(\"error_timestamp\", F.current_timestamp()) \\\n",
    "                           .write.format(\"delta\").mode(\"append\").saveAsTable(error_table)\n",
    "        print(f\"Logged {invalid_records} invalid records to error table\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error writing to staging table: {str(e)}\")\n",
    "    raise\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f3f464ba-2ee7-4574-99f7-0ed21c37d5bd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully archived 0 files\n"
     ]
    }
   ],
   "source": [
    "# Archive processed files\n",
    "try:\n",
    "    # List all files in the source directory\n",
    "    files = dbutils.fs.ls(source_dir)\n",
    "    \n",
    "    archived_count = 0\n",
    "    for file in files:\n",
    "        if file.name.endswith('.csv'):\n",
    "            src_path = file.path\n",
    "            archive_path = archive_dir + file.name\n",
    "            \n",
    "            # Move the file to archive\n",
    "            dbutils.fs.mv(src_path, archive_path)\n",
    "            archived_count += 1\n",
    "            print(f\"Archived: {file.name}\")\n",
    "    \n",
    "    print(f\"Successfully archived {archived_count} files\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error archiving files: {str(e)}\")\n",
    "    raise\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6e0306aa-5b01-4e88-8ee5-bf496e85309c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Summary:\n{\n  \"task\": \"customers_stage_load\",\n  \"timestamp\": \"2025-12-07T07:10:40.270870\",\n  \"total_records\": 0,\n  \"valid_records\": 0,\n  \"invalid_records\": 0,\n  \"archived_files\": 0,\n  \"status\": \"SUCCESS\"\n}\n"
     ]
    }
   ],
   "source": [
    "# Log processing summary\n",
    "processing_summary = {\n",
    "    \"task\": \"customers_stage_load\",\n",
    "    \"timestamp\": datetime.now().isoformat(),\n",
    "    \"total_records\": total_records,\n",
    "    \"valid_records\": valid_records,\n",
    "    \"invalid_records\": invalid_records,\n",
    "    \"archived_files\": archived_count,\n",
    "    \"status\": \"SUCCESS\" if invalid_records == 0 else \"SUCCESS_WITH_WARNINGS\"\n",
    "}\n",
    "\n",
    "print(\"Processing Summary:\")\n",
    "print(json.dumps(processing_summary, indent=2))\n",
    "\n",
    "# Store summary in a table for monitoring\n",
    "summary_df = spark.createDataFrame([processing_summary])\n",
    "summary_df.write.format(\"delta\").mode(\"append\").saveAsTable(\"`external-catlog-project`.default.processing_log\")\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "02_customers_stage_load",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}